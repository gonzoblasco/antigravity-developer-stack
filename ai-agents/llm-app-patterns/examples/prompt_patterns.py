from datetime import datetime

# --- 3.1 Prompt Templates with Variables ---

class PromptTemplate:
    def __init__(self, template: str, variables: list[str]):
        self.template = template
        self.variables = variables

    def format(self, **kwargs) -> str:
        # Validate all variables provided
        missing = set(self.variables) - set(kwargs.keys())
        if missing:
            raise ValueError(f"Missing variables: {missing}")

        return self.template.format(**kwargs)

    def with_examples(self, examples: list[dict]) -> str:
        """Add few-shot examples"""
        example_text = "\n\n".join([
            f"Input: {ex['input']}\nOutput: {ex['output']}"
            for ex in examples
        ])
        return f"{example_text}\n\n{self.template}"

# Usage
def usage_example():
    summarizer = PromptTemplate(
        template="Summarize the following text in {style} style:\n\n{text}",
        variables=["style", "text"]
    )

    prompt = summarizer.format(
        style="professional",
        text="Long article content..."
    )
    return prompt

# --- 3.2 Prompt Versioning & A/B Testing ---

class PromptRegistry:
    def __init__(self, db):
        self.db = db

    def register(self, name: str, template: str, version: str):
        """Store prompt with version"""
        self.db.save({
            "name": name,
            "template": template,
            "version": version,
            "created_at": datetime.now(),
            "metrics": {}
        })

    def get(self, name: str, version: str = "latest") -> str:
        """Retrieve specific version"""
        return self.db.get(name, version)

    def ab_test(self, name: str, user_id: str) -> str:
        """Return variant based on user bucket"""
        variants = self.db.get_all_versions(name)
        bucket = hash(user_id) % len(variants)
        return variants[bucket]

    def record_outcome(self, prompt_id: str, outcome: dict):
        """Track prompt performance"""
        self.db.update_metrics(prompt_id, outcome)

# --- 3.3 Prompt Chaining ---

class MockLLM:
    def generate(self, prompt): return "LLM Output"

llm = MockLLM()

class PromptChain:
    """
    Chain prompts together, passing output as input to next
    """

    def __init__(self, steps: list[dict]):
        self.steps = steps

    def run(self, initial_input: str) -> dict:
        context = {"input": initial_input}
        results = []

        for step in self.steps:
            prompt = step["prompt"].format(**context)
            output = llm.generate(prompt)

            # Parse output if needed
            if step.get("parser"):
                output = step["parser"](output)

            context[step["output_key"]] = output
            results.append({
                "step": step["name"],
                "output": output
            })

        return {
            "final_output": context[self.steps[-1]["output_key"]],
            "intermediate_results": results
        }

# Example: Research -> Analyze -> Summarize
def chain_example():
    chain = PromptChain([
        {
            "name": "research",
            "prompt": "Research the topic: {input}",
            "output_key": "research"
        },
        {
            "name": "analyze",
            "prompt": "Analyze these findings:\n{research}",
            "output_key": "analysis"
        },
        {
            "name": "summarize",
            "prompt": "Summarize this analysis in 3 bullet points:\n{analysis}",
            "output_key": "summary"
        }
    ])
    return chain
